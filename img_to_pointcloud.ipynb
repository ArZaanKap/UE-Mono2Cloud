{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbe147",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q accelerate open3d pillow laspy\n",
    "!pip install -q OpenEXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Apple's Depth Pro and dependencies\n",
    "!pip install git+https://github.com/apple/ml-depth-pro.git --quiet\n",
    "!pip install open3d --quiet\n",
    "!pip install timm --quiet\n",
    "\n",
    "# Download the checkpoint (if not handled automatically by the code)\n",
    "!wget https://huggingface.co/apple/DepthPro/resolve/main/depth_pro.pt -O depth_pro.pt\n",
    "!pip install \"numpy<2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4c62f",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# ==============================================================================\n# CONFIGURATION\n# ==============================================================================\n\n# --- MODEL SELECTION ---\n# Options: \"depth_anything_metric\", \"depth_anything_non_metric\", \"depth_pro\"\n# \n# MODEL COMPARISON (depth4 dataset, least_squares scaling):\n#   Depth Pro:              RMSE=0.055m, d1=100% (RECOMMENDED)\n#   Depth Anything Metric:  RMSE=0.084m, d1=99.9%\n#   Depth Pro is 35% more accurate!\nMODEL_TYPE = \"depth_pro\"\n\n# --- DATASET SELECTION ---\n# Available datasets in ./data/:\n#   - depth4: Room scene with edit.png (recommended)\n#   - depth5: Same as depth4\n#   - depth_gt_finder: Flat wall calibration test\n#   - mrq2: Movie Render Queue output (different format!)\nDATASET = \"depth4\"\n\nINPUT_FOLDER = f\"./data/{DATASET}\"\nOUTPUT_FOLDER = \"./output\"\n\n# --- FILE NAMING ---\n# Adjust these based on your dataset and UE export format\nif DATASET == \"mrq2\":\n    GT_IMG = \"Seq.FinalImage.0000.exr\"\n    GT_DEPTH_IMG = \"Seq.FinalImageMovieRenderQueue_WorldDepth.0000.exr\"\n    GT_TO_CENTIMETERS = 1.0  # MRQ WorldDepth is already in centimeters\nelse:\n    GT_IMG = \"HighresScreenshot00000.exr\"  # RGB image\n    GT_DEPTH_IMG = \"HighresScreenshot00000_SceneDepth.exr\"  # Depth file\n    # VERIFIED: SceneDepth * 10000 = centimeters (flat wall test confirmed)\n    GT_TO_CENTIMETERS = 10000.0\n\n# Optional: Use WorldUnits file instead (already in meters)\n# GT_DEPTH_IMG = \"HighresScreenshot00000_SceneDepthWorldUnits.exr\"\n# GT_TO_CENTIMETERS = 100.0  # WorldUnits * 100 = centimeters\n\n# Optional decorated/edited image for comparison\nDECORATED_IMG = \"edit.png\" if os.path.exists(f\"{INPUT_FOLDER}/edit.png\") else None\n\nDEPTH_PRO_CHECKPOINT = \"depth_pro.pt\"\n\n# Computed paths\nIMG_PATH = f\"{INPUT_FOLDER}/{GT_IMG}\"\nGT_DEPTH_PATH = f\"{INPUT_FOLDER}/{GT_DEPTH_IMG}\"\nDECORATED_PATH = f\"{INPUT_FOLDER}/{DECORATED_IMG}\" if DECORATED_IMG else None\n\nOUTPUT_LAS = f\"{OUTPUT_FOLDER}/room.las\"\nOUTPUT_LAS_DECORATED = f\"{OUTPUT_FOLDER}/room_decorated.las\"\n\n# Create output directory\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)\n\n# --- MODEL CONFIGURATIONS ---\nMODEL_CONFIGS = {\n    \"depth_anything_metric\": {\n        \"hf_id\": \"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\",\n        \"is_metric\": True,\n        \"type\": \"transformers\"\n    },\n    \"depth_anything_non_metric\": {\n        \"hf_id\": \"depth-anything/Depth-Anything-V2-Large-hf\",\n        \"is_metric\": False,\n        \"type\": \"transformers\"\n    },\n    \"depth_pro\": {\n        \"checkpoint\": DEPTH_PRO_CHECKPOINT,\n        \"is_metric\": True,\n        \"type\": \"depth_pro\"\n    }\n}\n\n# --- SCALING METHOD ---\n# Analysis results (depth4 dataset):\n#   \"none\":          Unusable (RMSE > 0.5m for both models)\n#   \"median\":        Good (RMSE ~0.07-0.14m)\n#   \"least_squares\": BEST (RMSE ~0.05-0.08m)\nSCALING_METHOD = \"least_squares\"\n\n# --- CAMERA ---\nCAMERA_FOV = 90.0  # Horizontal field of view in degrees (verified for UE camera)\n\n# --- FILTERING ---\nMIN_DEPTH = 0.1   # Minimum depth in meters\nMAX_DEPTH = 50.0  # Maximum depth in meters\n\n# --- POINT CLOUD CLEANING ---\nCLEAN_POINT_CLOUD = False  # Set to True to filter depth edges\nEDGE_THRESHOLD = 1.5       # Gradient threshold for edge detection\n\n# ==============================================================================\n# PRINT CONFIGURATION\n# ==============================================================================\nprint(f\"Configuration:\")\nprint(f\"  Dataset:    {DATASET}\")\nprint(f\"  Model:      {MODEL_TYPE}\")\nprint(f\"  Scaling:    {SCALING_METHOD}\")\nprint(f\"  GT->cm:     {GT_TO_CENTIMETERS}\")\nprint(f\"  Camera FOV: {CAMERA_FOV} deg\")\nprint(f\"  Cleaning:   {'Enabled' if CLEAN_POINT_CLOUD else 'Disabled'}\")\nprint(f\"\\nFiles:\")\nprint(f\"  RGB:        {IMG_PATH}\")\nprint(f\"  GT Depth:   {GT_DEPTH_PATH}\")\nprint(f\"  Decorated:  {DECORATED_PATH}\")\nprint(f\"  Output:     {OUTPUT_FOLDER}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e153d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import OpenEXR\n",
    "import Imath\n",
    "import os\n",
    "\n",
    "# Model-specific imports\n",
    "if MODEL_CONFIGS[MODEL_TYPE][\"type\"] == \"transformers\":\n",
    "    from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "elif MODEL_CONFIGS[MODEL_TYPE][\"type\"] == \"depth_pro\":\n",
    "    import depth_pro\n",
    "    import shutil\n",
    "\n",
    "import laspy  # For LAS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698761f2",
   "metadata": {},
   "outputs": [],
   "source": "def load_exr_rgb(path):\n    \"\"\"Load RGB channels from EXR file\"\"\"\n    exr_file = OpenEXR.InputFile(path)\n    header = exr_file.header()\n    dw = header['dataWindow']\n    width = dw.max.x - dw.min.x + 1\n    height = dw.max.y - dw.min.y + 1\n    \n    FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)\n    img_data = []\n    for c in ['R', 'G', 'B']:\n        channel_str = exr_file.channel(c, FLOAT)\n        channel = np.frombuffer(channel_str, dtype=np.float32).reshape(height, width)\n        img_data.append(channel)\n    \n    img = np.stack(img_data, axis=-1)\n    img = np.clip(img, 0, 1)\n    img = (img * 255).astype(np.uint8)\n    return Image.fromarray(img)\n\n\ndef load_exr_depth(path):\n    \"\"\"Load depth channel from EXR file safely checking for Z vs R\"\"\"\n    exr_file = OpenEXR.InputFile(path)\n    header = exr_file.header()\n    dw = header['dataWindow']\n    width = dw.max.x - dw.min.x + 1\n    height = dw.max.y - dw.min.y + 1\n    \n    # Check channels: Prefer 'Z' (Planar) or 'SceneDepth' over 'R'\n    channels = header['channels'].keys()\n    if 'Z' in channels:\n        print(f\"  [EXR] Using 'Z' channel (Planar) for {os.path.basename(path)}\")\n        chan_name = 'Z'\n    elif 'SceneDepth' in channels:\n        print(f\"  [EXR] Using 'SceneDepth' channel for {os.path.basename(path)}\")\n        chan_name = 'SceneDepth'\n    else:\n        print(f\"  [EXR] Warning: Using 'R' channel. It is planar distance.\")\n        chan_name = 'R'\n\n    FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)\n    channel_str = exr_file.channel(chan_name, FLOAT)\n    \n    depth = np.frombuffer(channel_str, dtype=np.float32).reshape(height, width).copy()\n    \n    # Sanity check for infinite/negative values\n    depth[depth == np.inf] = 0\n    depth[depth < 0] = 0\n    return depth\n\n\ndef load_image(path):\n    \"\"\"Load image from EXR, PNG, or JPG\"\"\"\n    ext = os.path.splitext(path)[1].lower()\n    \n    if ext == '.exr':\n        return load_exr_rgb(path)\n    elif ext in ['.png', '.jpg', '.jpeg']:\n        return Image.open(path).convert(\"RGB\")\n    else:\n        raise ValueError(f\"Unsupported file format: {ext}\")\n\n\ndef align_depth(pred_depth, gt_depth, method=\"median\"):\n    \"\"\"Align predicted depth to ground truth using median, least squares, or none\"\"\"\n    valid_mask = (gt_depth > 0) & (~np.isnan(gt_depth)) & (pred_depth > 0)\n    gt_valid = gt_depth[valid_mask]\n    pred_valid = pred_depth[valid_mask]\n    \n    if method == \"none\":\n        # Crucial for Metric models (Depth Pro / Depth Anything V2 Metric)\n        print(\"Alignment: None (Metric Mode - keeping raw values)\")\n        scale = 1.0\n        shift = 0.0\n        aligned = pred_depth\n    elif method == \"median\":\n        scale = np.median(gt_valid) / np.median(pred_valid)\n        shift = 0.0\n        aligned = pred_depth * scale\n        print(f\"Alignment: Median scaling (x{scale:.4f})\")\n    elif method == \"least_squares\":\n        A = np.vstack([pred_valid, np.ones(len(pred_valid))]).T\n        scale, shift = np.linalg.lstsq(A, gt_valid, rcond=None)[0]\n        aligned = (pred_depth * scale) + shift\n        print(f\"Alignment: Least squares (x{scale:.4f} + {shift:.4f})\")\n    else:\n        raise ValueError(f\"Unknown scaling method: {method}\")\n    \n    return aligned, scale, shift, valid_mask\n\n\ndef compute_metrics(pred_aligned, gt_depth, valid_mask):\n    \"\"\"Compute depth estimation metrics: RMSE, MAE, relative error, delta accuracy, log RMSE\"\"\"\n    gt = gt_depth[valid_mask]\n    pred = pred_aligned[valid_mask]\n    \n    # Standard Errors\n    abs_diff = np.abs(gt - pred)\n    mae = np.mean(abs_diff)\n    rmse = np.sqrt(np.mean(abs_diff ** 2))\n    \n    # Relative Error\n    rel_error = np.mean(abs_diff / gt) * 100\n    \n    # Accuracy Thresholds (Delta metrics)\n    thresh = np.maximum((gt / (pred + 1e-6)), (pred / (gt + 1e-6)))\n    delta1 = (thresh < 1.25).mean()\n    delta2 = (thresh < 1.25 ** 2).mean()\n    delta3 = (thresh < 1.25 ** 3).mean()\n    \n    # Log RMSE (Penalizes errors at close range more heavily)\n    rmse_log = np.sqrt(np.mean((np.log(gt + 1e-6) - np.log(pred + 1e-6))**2))\n    \n    return rmse, mae, rel_error, delta1, rmse_log\n\n\nprint(\"Utility functions loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b198cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device.upper()}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "if config[\"type\"] == \"transformers\":\n",
    "    print(f\"Loading {MODEL_TYPE}...\")\n",
    "    processor = AutoImageProcessor.from_pretrained(config[\"hf_id\"])\n",
    "    model = AutoModelForDepthEstimation.from_pretrained(config[\"hf_id\"]).to(device)\n",
    "    print(f\"✓ Model loaded: {config['hf_id']}\")\n",
    "    \n",
    "elif config[\"type\"] == \"depth_pro\":\n",
    "    print(f\"Loading Depth Pro...\")\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    shutil.copy(config[\"checkpoint\"], \"checkpoints/depth_pro.pt\")\n",
    "    model, transform = depth_pro.create_model_and_transforms(\n",
    "        device=device, \n",
    "        precision=torch.float16\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"✓ Depth Pro loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da516fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading images...\")\n",
    "image = load_image(IMG_PATH)\n",
    "gt_depth = load_exr_depth(GT_DEPTH_PATH)\n",
    "h_gt, w_gt = gt_depth.shape\n",
    "\n",
    "if config[\"type\"] == \"transformers\":\n",
    "    print(\"Running depth estimation...\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_depth = model(**inputs).predicted_depth.squeeze().cpu().numpy()\n",
    "\n",
    "    print(f\"gt shape: {gt_depth.shape}\")\n",
    "    print(f\"pred shape: {pred_depth.shape}\")\n",
    "    \n",
    "    # Resize to match GT\n",
    "    if pred_depth.shape == gt_depth.shape:\n",
    "        print(\"shapes align - skipping resize\")\n",
    "    else:\n",
    "        pred_depth = cv2.resize(pred_depth, (w_gt, h_gt), interpolation=cv2.INTER_LINEAR)\n",
    "        print(\"pred_depth resized!\")\n",
    "    \n",
    "    # Invert if non-metric model (disparity → depth)\n",
    "    if not config[\"is_metric\"]:\n",
    "        print(\"⚠️ Non-metric model - inverting disparity to depth\")\n",
    "        pred_depth = 1.0 / (pred_depth + 1e-6)\n",
    "\n",
    "elif config[\"type\"] == \"depth_pro\":\n",
    "    print(\"Running Depth Pro inference...\")\n",
    "    image_tensor = transform(image)\n",
    "    with torch.no_grad():\n",
    "        prediction = model.infer(image_tensor, f_px=None)\n",
    "    \n",
    "    pred_depth = prediction[\"depth\"].squeeze().cpu().numpy()\n",
    "    \n",
    "    # Resize to match GT\n",
    "    pred_depth = cv2.resize(pred_depth, (w_gt, h_gt), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "print(f\"✓ Depth estimated (shape: {pred_depth.shape})\")\n",
    "\n",
    "\n",
    "\n",
    "# ← NEW: Run on decorated image if provided\n",
    "decorated_depth = None\n",
    "if DECORATED_PATH:\n",
    "    decorated_image = load_image(DECORATED_PATH)\n",
    "    w_dec, h_dec = decorated_image.size  # ← Keep native resolution\n",
    "    if config[\"type\"] == \"transformers\":\n",
    "        inputs_dec = processor(images=decorated_image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            decorated_depth = model(**inputs_dec).predicted_depth.squeeze().cpu().numpy()\n",
    "        decorated_depth = cv2.resize(decorated_depth, (w_dec, h_dec), interpolation=cv2.INTER_LINEAR)\n",
    "        if not config[\"is_metric\"]:\n",
    "            decorated_depth = 1.0 / (decorated_depth + 1e-6)\n",
    "    elif config[\"type\"] == \"depth_pro\":\n",
    "        image_tensor_dec = transform(decorated_image)\n",
    "        with torch.no_grad():\n",
    "            prediction_dec = model.infer(image_tensor_dec, f_px=None)\n",
    "        decorated_depth = prediction_dec[\"depth\"].squeeze().cpu().numpy()\n",
    "        decorated_depth = cv2.resize(decorated_depth, (w_dec, h_dec), interpolation=cv2.INTER_LINEAR)\n",
    "    print(f\"✓ Decorated depth estimated (shape: {decorated_depth.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_aligned, scale, shift, valid_mask = align_depth(pred_depth, gt_depth, SCALING_METHOD)#############\n",
    "#rmse, mae, rel_error = compute_metrics(pred_aligned, gt_depth, valid_mask)\n",
    "rmse, mae, rel_error, delta1, rmse_log = compute_metrics(pred_aligned, gt_depth, valid_mask)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS ({MODEL_TYPE.upper()} - {SCALING_METHOD})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Scale:      {scale:.4f}\")\n",
    "if SCALING_METHOD == \"least_squares\":\n",
    "    print(f\"  Shift:      {shift:.4f}\")\n",
    "print(f\"  RMSE:       {rmse:.6f}\")\n",
    "print(f\"  MAE:        {mae:.6f}\")\n",
    "print(f\"  Rel Error:  {rel_error:.4f}%\\n\")\n",
    "\n",
    "print(f\"  delta1:     {delta1:.4f}%\")\n",
    "print(f\"  RMSE log:   {rmse_log:.6f}\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d84c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "\n",
    "vmax = np.percentile(gt_depth[valid_mask], 95)\n",
    "gt_viz = np.clip(gt_depth, 0, vmax)\n",
    "pred_viz = np.clip(pred_aligned, 0, vmax)\n",
    "error_viz = np.clip(np.abs(gt_depth - pred_aligned), 0, mae * 3)\n",
    "error_viz[~valid_mask] = 0\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Input RGB\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(gt_viz, cmap='magma', vmin=0, vmax=vmax)\n",
    "plt.title(\"Ground Truth Depth\")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(pred_viz, cmap='magma', vmin=0, vmax=vmax)\n",
    "plt.title(f\"Predicted Depth (×{scale:.5f})\")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(error_viz, cmap='inferno')\n",
    "plt.title(f\"Absolute Error (MAE: {mae:.6f})\")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ← NEW: Show decorated depth if available\n",
    "if decorated_depth is not None:\n",
    "    decorated_aligned = decorated_depth * scale\n",
    "    decorated_viz = np.clip(decorated_aligned, 0, vmax)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(decorated_image)\n",
    "    plt.title(\"Decorated RGB\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(decorated_viz, cmap='magma', vmin=0, vmax=vmax)\n",
    "    plt.title(f\"Decorated Depth (×{scale:.5f})\")\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    # Resize to GT res just for this diff comparison\n",
    "    dec_aligned_resized = cv2.resize(decorated_aligned, (w_gt, h_gt), interpolation=cv2.INTER_LINEAR)\n",
    "    depth_diff = np.abs(dec_aligned_resized - pred_aligned)\n",
    "    plt.imshow(depth_diff, cmap='viridis')\n",
    "    plt.title(\"Depth Difference (Decorated - Original)\")\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33716a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1. PREPARE VISUALIZATION DATA\n",
    "# =========================================\n",
    "# Create error maps\n",
    "abs_diff = np.abs(gt_depth - pred_aligned)\n",
    "signed_diff = pred_aligned - gt_depth\n",
    "rel_diff = abs_diff / (gt_depth + 1e-6)\n",
    "\n",
    "# Mask out invalid pixels for cleaner plots\n",
    "abs_diff[~valid_mask] = 0\n",
    "signed_diff[~valid_mask] = 0\n",
    "rel_diff[~valid_mask] = 0\n",
    "\n",
    "# Set dynamic limits based on percentiles (ignores outliers)\n",
    "depth_vmax = np.percentile(gt_depth[valid_mask], 95)\n",
    "err_vmax = np.percentile(abs_diff[valid_mask], 95)\n",
    "signed_vmax = max(np.percentile(np.abs(signed_diff[valid_mask]), 95), 1e-3)\n",
    "\n",
    "# =========================================\n",
    "# 2. MAIN DASHBOARD (2 Rows x 4 Columns)\n",
    "# =========================================\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "# --- ROW 1: Qualitative (Visuals) ---\n",
    "\n",
    "# 1. Input RGB\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Input RGB\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "\n",
    "# 2. Ground Truth\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.imshow(gt_depth, cmap='magma', vmin=0, vmax=depth_vmax)\n",
    "plt.title(\"Ground Truth Depth\", fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label=\"Depth\", shrink=0.6)\n",
    "plt.axis('off')\n",
    "\n",
    "# 3. Prediction\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.imshow(pred_aligned, cmap='magma', vmin=0, vmax=depth_vmax)\n",
    "plt.title(f\"Predicted Depth\\n(Scaled x{scale:.4f})\", fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label=\"Depth\", shrink=0.6)\n",
    "plt.axis('off')\n",
    "\n",
    "# 4. Absolute Error (Heatmap)\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.imshow(abs_diff, cmap='inferno', vmin=0, vmax=err_vmax)\n",
    "plt.title(f\"Absolute Error\\n(MAE: {mae:.4f})\", fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label=\"Abs Diff\", shrink=0.6)\n",
    "plt.axis('off')\n",
    "\n",
    "# --- ROW 2: Quantitative (Analytics) ---\n",
    "\n",
    "# 5. Signed Error (Bias Check)\n",
    "# Red = Prediction is too far, Blue = Prediction is too close\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.imshow(signed_diff, cmap='RdBu_r', vmin=-signed_vmax, vmax=signed_vmax)\n",
    "plt.title(\"Signed Error (Bias)\\nRed = Overest., Blue = Underest.\", fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label=\"Pred - GT\", shrink=0.6)\n",
    "plt.axis('off')\n",
    "\n",
    "# 6. Relative Error\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.imshow(rel_diff, cmap='Reds', vmin=0, vmax=0.5) # Cap at 50% error for visibility\n",
    "plt.title(f\"Relative Error\\n(AbsRel: {rel_error:.2f}%)\", fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label=\"Rel Error\", shrink=0.6)\n",
    "plt.axis('off')\n",
    "\n",
    "# 7. Error Histogram\n",
    "plt.subplot(2, 4, 7)\n",
    "plt.hist(abs_diff[valid_mask], bins=50, range=(0, err_vmax), color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel(\"Absolute Error\")\n",
    "plt.ylabel(\"Pixel Count\")\n",
    "plt.title(\"Error Distribution\", fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 8. Metrics Summary Box\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.axis('off')\n",
    "metrics_text = (\n",
    "    f\"MODEL METRICS\\n\"\n",
    "    f\"----------------------\\n\"\n",
    "    f\"RMSE:     {rmse:.4f}\\n\"\n",
    "    f\"MAE:      {mae:.4f}\\n\"\n",
    "    f\"AbsRel:   {rel_error:.2f}%\\n\"\n",
    "    f\"----------------------\\n\"\n",
    "    f\"ACCURACY (δ)\\n\"\n",
    "    f\"δ < 1.25: {delta1*100:.2f}%\\n\"\n",
    "    f\"----------------------\\n\"\n",
    "    f\"SCALING\\n\"\n",
    "    f\"Method:   {SCALING_METHOD}\\n\"\n",
    "    f\"Scale:    {scale:.4f}\"\n",
    ")\n",
    "plt.text(0.5, 0.5, metrics_text, fontsize=16, family='monospace', va='center', ha='center',\n",
    "         bbox=dict(boxstyle=\"round,pad=1\", facecolor='whitesmoke', edgecolor='gray', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 3. DECORATED COMPARISON (Keep as separate block)\n",
    "# =========================================\n",
    "if decorated_depth is not None:\n",
    "    decorated_aligned = decorated_depth * scale\n",
    "    \n",
    "    # Resize decorated to GT size for accurate diff\n",
    "    dec_aligned_resized = cv2.resize(decorated_aligned, (w_gt, h_gt), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Difference: How much did the decoration change the depth?\n",
    "    change_map = dec_aligned_resized - pred_aligned\n",
    "    change_vmax = max(np.percentile(np.abs(change_map), 99), 1e-3)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(decorated_image)\n",
    "    plt.title(\"Decorated Input\", fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(decorated_aligned, cmap='magma', vmin=0, vmax=depth_vmax)\n",
    "    plt.title(\"Decorated Depth Prediction\", fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(change_map, cmap='RdBu_r', vmin=-change_vmax, vmax=change_vmax)\n",
    "    plt.title(\"Impact of Decoration\\n(Decorated - Original)\", fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(label=\"Depth Change\", shrink=0.8)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c73bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating point cloud...\")\n",
    "\n",
    "# Convert depth to centimeters then meters\n",
    "depth_cm = pred_aligned * GT_TO_CENTIMETERS   # already multiplied by scale + shift\n",
    "depth_m = depth_cm / 100.0\n",
    "\n",
    "print(f\"Depth range: {depth_m.min():.2f}m to {depth_m.max():.2f}m (mean: {depth_m.mean():.2f}m)\")\n",
    "\n",
    "# Compute focal length from FOV\n",
    "h, w = depth_m.shape\n",
    "focal_length = w / (2 * np.tan(np.radians(CAMERA_FOV / 2)))\n",
    "print(f\"Focal length from FOV ({CAMERA_FOV}°): {focal_length:.2f}px\")\n",
    "\n",
    "# Back-project to 3D\n",
    "cx, cy = w / 2, h / 2\n",
    "xx, yy = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "z = depth_m.flatten()\n",
    "x = (xx.flatten() - cx) * z / focal_length\n",
    "y = (yy.flatten() - cy) * z / focal_length\n",
    "\n",
    "# Get colors\n",
    "rgb = np.array(image.resize((w, h))).reshape(-1, 3)\n",
    "\n",
    "# Filter by depth range\n",
    "mask = (z >= MIN_DEPTH) & (z <= MAX_DEPTH) & (z > 0)\n",
    "\n",
    "# Optional edge filtering\n",
    "if CLEAN_POINT_CLOUD:\n",
    "    print(\"Applying edge filtering...\")\n",
    "    depth_grad_x = cv2.Sobel(depth_m, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    depth_grad_y = cv2.Sobel(depth_m, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(depth_grad_x**2 + depth_grad_y**2)\n",
    "    edge_mask = (grad_mag < EDGE_THRESHOLD).flatten()\n",
    "    mask = mask & edge_mask\n",
    "\n",
    "x, y, z, rgb = x[mask], y[mask], z[mask], rgb[mask]\n",
    "\n",
    "print(f\"✓ Generated {len(z):,} points\")\n",
    "\n",
    "\n",
    "# ← NEW: Generate decorated point cloud if available\n",
    "if decorated_depth is not None:\n",
    "    print(\"\\nGenerating decorated point cloud...\")\n",
    "    \n",
    "    # Convert depth to centimeters then meters (use same scale)\n",
    "    depth_cm_dec = (decorated_depth * scale + shift) * GT_TO_CENTIMETERS   # shift 0 for median\n",
    "    depth_m_dec = depth_cm_dec / 100.0\n",
    "    \n",
    "    print(f\"Decorated depth range: {depth_m_dec.min():.2f}m to {depth_m_dec.max():.2f}m (mean: {depth_m_dec.mean():.2f}m)\")\n",
    "\n",
    "    # Recompute grid at decorated image's own resolution\n",
    "    h_dec, w_dec = depth_m_dec.shape\n",
    "    focal_length_dec = w_dec / (2 * np.tan(np.radians(CAMERA_FOV / 2)))\n",
    "    cx_dec, cy_dec = w_dec / 2, h_dec / 2\n",
    "    xx_dec, yy_dec = np.meshgrid(np.arange(w_dec), np.arange(h_dec))\n",
    "    \n",
    "    # Back-project to 3D\n",
    "    z_dec = depth_m_dec.flatten()\n",
    "    x_dec = (xx_dec.flatten() - cx_dec) * z_dec / focal_length_dec\n",
    "    y_dec = (yy_dec.flatten() - cy_dec) * z_dec / focal_length_dec\n",
    "    \n",
    "    # Get colors from decorated image\n",
    "    rgb_dec = np.array(decorated_image).reshape(-1, 3)\n",
    "    \n",
    "    # Filter by depth range\n",
    "    mask_dec = (z_dec >= MIN_DEPTH) & (z_dec <= MAX_DEPTH) & (z_dec > 0)\n",
    "    \n",
    "    # Optional edge filtering\n",
    "    if CLEAN_POINT_CLOUD:\n",
    "        print(\"Applying edge filtering to decorated...\")\n",
    "        depth_grad_x_dec = cv2.Sobel(depth_m_dec, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        depth_grad_y_dec = cv2.Sobel(depth_m_dec, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        grad_mag_dec = np.sqrt(depth_grad_x_dec**2 + depth_grad_y_dec**2)\n",
    "        edge_mask_dec = (grad_mag_dec < EDGE_THRESHOLD).flatten()\n",
    "        mask_dec = mask_dec & edge_mask_dec\n",
    "    \n",
    "    x_dec, y_dec, z_dec, rgb_dec = x_dec[mask_dec], y_dec[mask_dec], z_dec[mask_dec], rgb_dec[mask_dec]\n",
    "    \n",
    "    print(f\"✓ Generated {len(z_dec):,} decorated points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving point cloud...\")\n",
    "\n",
    "header = laspy.LasHeader(point_format=3, version=\"1.2\")\n",
    "header.scales = np.array([0.001, 0.001, 0.001])\n",
    "las = laspy.LasData(header=header)\n",
    "\n",
    "# LAS coordinate system: Z forward, -X right, -Y up\n",
    "las.x = z\n",
    "las.y = -x\n",
    "las.z = -y\n",
    "las.red = rgb[:, 0].astype(np.uint16) * 256\n",
    "las.green = rgb[:, 1].astype(np.uint16) * 256\n",
    "las.blue = rgb[:, 2].astype(np.uint16) * 256\n",
    "\n",
    "las.write(OUTPUT_LAS)\n",
    "print(f\"✓ Saved {len(z):,} points to {OUTPUT_LAS}\")\n",
    "\n",
    "\n",
    "# ← NEW: Save decorated point cloud if available\n",
    "if decorated_depth is not None:\n",
    "    print(\"\\nSaving decorated point cloud...\")\n",
    "    \n",
    "    header_dec = laspy.LasHeader(point_format=3, version=\"1.2\")\n",
    "    header_dec.scales = np.array([0.001, 0.001, 0.001])\n",
    "    las_dec = laspy.LasData(header_dec)\n",
    "    \n",
    "    # LAS coordinate system: Z forward, -X right, -Y up\n",
    "    las_dec.x = z_dec\n",
    "    las_dec.y = -x_dec\n",
    "    las_dec.z = -y_dec\n",
    "    las_dec.red = rgb_dec[:, 0].astype(np.uint16) * 256\n",
    "    las_dec.green = rgb_dec[:, 1].astype(np.uint16) * 256\n",
    "    las_dec.blue = rgb_dec[:, 2].astype(np.uint16) * 256\n",
    "    \n",
    "    las_dec.write(OUTPUT_LAS_DECORATED)\n",
    "    print(f\"✓ Saved {len(z_dec):,} decorated points to {OUTPUT_LAS_DECORATED}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}