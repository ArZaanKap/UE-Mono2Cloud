{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbe147",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport cv2\nimport OpenEXR, Imath\nimport depth_pro\nimport laspy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom scipy.stats import skew\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator\nfrom segment_anything.utils.transforms import ResizeLongestSide\nimport matplotlib.pyplot as plt\n\n# ── Config ──\nDATASET = \"depth4\"\nINPUT_FOLDER = f\"./data/{DATASET}\"\nOUTPUT_FOLDER = \"./output\"\nGT_TO_CENTIMETERS = 10000.0\nCAMERA_FOV = 90.0\nMIN_DEPTH, MAX_DEPTH = 0.1, 50.0\n\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device.upper()}, Dataset: {DATASET}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079de55",
   "metadata": {},
   "outputs": [],
   "source": "def load_exr_rgb(path):\n    exr = OpenEXR.InputFile(path)\n    dw = exr.header()['dataWindow']\n    w, h = dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1\n    FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)\n    rgb = np.stack([np.frombuffer(exr.channel(c, FLOAT), np.float32).reshape(h, w) for c in 'RGB'], axis=-1)\n    return Image.fromarray(np.clip(rgb * 255, 0, 255).astype(np.uint8))\n\ndef load_exr_depth(path):\n    exr = OpenEXR.InputFile(path)\n    header = exr.header()\n    dw = header['dataWindow']\n    w, h = dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1\n    FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)\n    channels = list(header['channels'].keys())\n    for name in ['R', 'SceneDepth', 'Z']:\n        if name in channels:\n            depth = np.frombuffer(exr.channel(name, FLOAT), np.float32).reshape(h, w).copy()\n            break\n    depth_m = (depth * GT_TO_CENTIMETERS) / 100.0\n    return depth_m\n\ndef load_image(path):\n    return load_exr_rgb(path) if path.lower().endswith('.exr') else Image.open(path).convert('RGB')\n\n# ── Find files ──\nfiles = os.listdir(INPUT_FOLDER)\ngt_rgb_path = next(os.path.join(INPUT_FOLDER, f) for f in files if f.endswith('.exr') and 'depth' not in f.lower() and 'scenedepth' not in f.lower())\nedited_path = next(os.path.join(INPUT_FOLDER, f) for f in files if 'edit' in f.lower() and f.endswith(('.png', '.jpg', '.exr')))\ngt_depth_path = next(os.path.join(INPUT_FOLDER, f) for f in files if 'SceneDepth' in f and 'WorldUnits' not in f and f.endswith('.exr'))\n\nprint(f\"Original: {os.path.basename(gt_rgb_path)}\")\nprint(f\"Edited:   {os.path.basename(edited_path)}\")\nprint(f\"GT Depth: {os.path.basename(gt_depth_path)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4c62f",
   "metadata": {},
   "outputs": [],
   "source": "# ── Load all data ──\noriginal_img = load_image(gt_rgb_path)\nedited_img = load_image(edited_path)\ngt_depth = load_exr_depth(gt_depth_path)\nh_gt, w_gt = gt_depth.shape\n\n# Resize edited to match original for change detection\nedited_resized = edited_img.resize(original_img.size, Image.BILINEAR) if edited_img.size != original_img.size else edited_img\n\nprint(f\"Original: {original_img.size}, Edited: {edited_img.size}\")\nprint(f\"GT depth: {gt_depth.shape}, range: {gt_depth.min():.2f}m - {gt_depth.max():.2f}m\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e153d9",
   "metadata": {},
   "outputs": [],
   "source": "# ── Change detection (GeSCF: SAM Q/K/V attention features + adaptive threshold) ──\nweights_dir = \"./weights\"\nos.makedirs(weights_dir, exist_ok=True)\nweights_path = os.path.join(weights_dir, \"sam_vit_b_01ec64.pth\")\n\nif not os.path.exists(weights_path):\n    print(\"Downloading SAM ViT-B weights (~375 MB)...\")\n    import urllib.request\n    urllib.request.urlretrieve(\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", weights_path)\n\nsam = sam_model_registry[\"vit_b\"](checkpoint=weights_path).to(device).eval()\nh_img, w_img = np.array(original_img).shape[:2]\n\n# Hook into block 8 (global attention) to capture Q/K/V features\ncaptured = {}\ndef _hook_qkv(module, input, output):\n    captured[\"qkv\"] = output.detach()\nhook = sam.image_encoder.blocks[8].attn.qkv.register_forward_hook(_hook_qkv)\n\n# Preprocess both images for SAM\nimg_size = sam.image_encoder.img_size\nsam_transform = ResizeLongestSide(img_size)\npatch_size = 16\nsam_scale = img_size / max(h_img, w_img)\nfeat_h, feat_w = int(h_img * sam_scale + 0.5) // patch_size, int(w_img * sam_scale + 0.5) // patch_size\n\ndef prepare_sam(pil_img):\n    t = sam_transform.apply_image(np.array(pil_img))\n    return sam.preprocess(torch.as_tensor(t, device=device).permute(2, 0, 1).unsqueeze(0).float())\n\n# Extract features and compute cosine distance\nwith torch.no_grad():\n    sam.image_encoder(prepare_sam(original_img))\n    qkv1 = captured[\"qkv\"]\n    sam.image_encoder(prepare_sam(edited_resized))\n    qkv2 = captured[\"qkv\"]\nhook.remove()\n\nwith torch.no_grad():\n    f1, f2 = qkv1.squeeze(0), qkv2.squeeze(0)\n    cos_sim = (F.normalize(f1, dim=-1) * F.normalize(f2, dim=-1)).sum(dim=-1)\n    dist_map = (1 - cos_sim).cpu().numpy()[:feat_h, :feat_w]\n\n# Upsample, smooth, normalize\ndist_map = F.interpolate(torch.tensor(dist_map)[None, None].float(), size=(h_img, w_img), mode='bilinear', align_corners=False).squeeze().numpy()\ndist_map = ndimage.gaussian_filter(dist_map, sigma=4)\ndist_map = (dist_map - dist_map.min()) / (dist_map.max() - dist_map.min() + 1e-8)\n\n# Adaptive threshold (skewness-based, from GeSCF paper)\nsk = skew(dist_map.ravel())\nk = np.clip(sk, 1.0, 3.0)\nthreshold = dist_map.mean() + k * dist_map.std()\ninitial_mask = dist_map > threshold\n\n# SAM segment-level refinement\nmask_gen = SamAutomaticMaskGenerator(sam, points_per_side=16, pred_iou_thresh=0.80, stability_score_thresh=0.85, min_mask_region_area=200)\nsam_masks = mask_gen.generate(np.array(edited_resized))\nchanged_mask = np.zeros((h_img, w_img), dtype=bool)\nfor seg in sam_masks:\n    m = seg[\"segmentation\"]\n    if m.sum() > 0 and np.logical_and(m, initial_mask).sum() / m.sum() > 0.3:\n        changed_mask |= m\nif changed_mask.sum() == 0:\n    changed_mask = initial_mask\n\n# Free SAM memory\ndel sam, mask_gen, qkv1, qkv2\ntorch.cuda.empty_cache()\n\n# Resize mask to GT depth resolution\nif changed_mask.shape != gt_depth.shape:\n    changed_mask = cv2.resize(changed_mask.astype(np.uint8), (w_gt, h_gt), interpolation=cv2.INTER_NEAREST) > 0\nunchanged_mask = ~changed_mask\n\nprint(f\"Changed pixels: {changed_mask.sum():,} ({changed_mask.mean()*100:.1f}%)\")\nprint(f\"Threshold: {threshold:.4f} (skew={sk:.2f}, k={k:.2f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698761f2",
   "metadata": {},
   "outputs": [],
   "source": "# ── Run Depth Pro on edited image ──\nmodel, transform = depth_pro.create_model_and_transforms(device=device)\nmodel.eval()\n\nwith torch.no_grad():\n    pred = model.infer(transform(edited_img), f_px=None)\npred_depth = pred[\"depth\"].squeeze().cpu().numpy()\npred_depth = cv2.resize(pred_depth, (w_gt, h_gt), interpolation=cv2.INTER_LINEAR)\n\ndel model\ntorch.cuda.empty_cache()\nprint(f\"Predicted depth: {pred_depth.shape}, range: {pred_depth.min():.2f}m - {pred_depth.max():.2f}m\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b198cf7",
   "metadata": {},
   "outputs": [],
   "source": "# ── Least squares scaling on unchanged regions ──\nvalid = unchanged_mask & (gt_depth > 0.1) & (gt_depth < 100) & np.isfinite(pred_depth) & np.isfinite(gt_depth)\nA = np.vstack([pred_depth[valid].flatten(), np.ones(valid.sum())]).T\nscale, shift = np.linalg.lstsq(A, gt_depth[valid].flatten(), rcond=None)[0]\ndepth_scaled = pred_depth * scale + shift\n\nmae = np.mean(np.abs(depth_scaled[unchanged_mask] - gt_depth[unchanged_mask]))\nrmse = np.sqrt(np.mean((depth_scaled[unchanged_mask] - gt_depth[unchanged_mask]) ** 2))\nprint(f\"LS fit: scale={scale:.4f}, shift={shift:.4f}\")\nprint(f\"Unchanged regions — MAE: {mae:.4f}m, RMSE: {rmse:.4f}m\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da516fb",
   "metadata": {},
   "outputs": [],
   "source": "# ── Visualization ──\nvmax = gt_depth.max()\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\naxes[0].imshow(edited_img); axes[0].set_title('Edited Image'); axes[0].axis('off')\naxes[1].imshow(gt_depth, cmap='turbo', vmin=0, vmax=vmax); axes[1].set_title('GT Depth'); axes[1].axis('off')\naxes[2].imshow(depth_scaled, cmap='turbo', vmin=0, vmax=vmax); axes[2].set_title('Scaled Prediction'); axes[2].axis('off')\n\nerror = np.abs(depth_scaled - gt_depth) * unchanged_mask\naxes[3].imshow(error, cmap='inferno', vmin=0, vmax=0.5); axes[3].set_title(f'Error (MAE={mae:.3f}m)'); axes[3].axis('off')\n\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "code",
   "id": "qxau96pq13g",
   "source": "# ── Generate point cloud ──\nw_edit, h_edit = edited_img.size\ndepth_full = cv2.resize(depth_scaled, (w_edit, h_edit), interpolation=cv2.INTER_LINEAR)\n\nfocal = w_edit / (2 * np.tan(np.radians(CAMERA_FOV / 2)))\ncx, cy = w_edit / 2, h_edit / 2\nxx, yy = np.meshgrid(np.arange(w_edit), np.arange(h_edit))\n\nz = depth_full.flatten()\nx = (xx.flatten() - cx) * z / focal\ny = (yy.flatten() - cy) * z / focal\nrgb = np.array(edited_img).reshape(-1, 3)\n\nmask = (z >= MIN_DEPTH) & (z <= MAX_DEPTH)\nx, y, z, rgb = x[mask], y[mask], z[mask], rgb[mask]\nprint(f\"Point cloud: {len(z):,} points\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79d67e",
   "metadata": {},
   "outputs": [],
   "source": "# ── Save as LAS ──\noutput_path = f\"{OUTPUT_FOLDER}/room_edited.las\"\n\nheader = laspy.LasHeader(point_format=3, version=\"1.2\")\nheader.scales = np.array([0.001, 0.001, 0.001])\nlas = laspy.LasData(header=header)\n\nlas.x = z       # forward\nlas.y = -x      # right\nlas.z = -y      # up\nlas.red = rgb[:, 0].astype(np.uint16) * 256\nlas.green = rgb[:, 1].astype(np.uint16) * 256\nlas.blue = rgb[:, 2].astype(np.uint16) * 256\n\nlas.write(output_path)\nprint(f\"Saved {len(z):,} points to {output_path}\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}